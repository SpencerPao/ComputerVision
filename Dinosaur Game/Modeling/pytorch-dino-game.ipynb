{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb3fbf3-eb58-4a63-bc58-adccbc6eb32b",
   "metadata": {},
   "source": [
    "# Dino Game model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10ea9d-7a20-4b89-a0ec-72766ec977e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summary\n",
    "* PyTorch DNN to play Dino-Game\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* 1D vectors of cropped screenshots of dino games\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* A label prediction `[0, 1, 2]` for the key to press (or what action the model should take given the pixel values in the image)\n",
    "\n",
    "### Modeling task\n",
    "* Given an input image $X$, output a label for the action to be taken by the model (jump, duck, nothing)\n",
    "\n",
    "### Evaluation metric\n",
    "* Classification accuracy \n",
    "* Cross-entropy loss for training\n",
    "\n",
    "### Models\n",
    "* Multinomial logistic/Softmax regression in other notebooks\n",
    "* Deep Feed-Forward NNet with ReLU activations and a softmax output layer\n",
    "\n",
    "### To-do\n",
    "* Well this model seems to predcict well without overfititng in training --- can't really run things locally after the google collab issues and all but hopefully this performs better than LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65e5134-9a96-4b39-ae95-549873eb26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import pickle\n",
    "# Pre-processing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io, transform\n",
    "\n",
    "# Numerical packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# Plotting & eval metrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684f1aae-dc23-4dc8-b8a8-1dce047c67dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_keys(label_vec):\n",
    "    \"\"\"Take a vector of key-press labels and convert them to proper encodings. \"\"\"\n",
    "    result = np.zeros_like(label_vec)\n",
    "    key_dict = {-1: 0, 38: 1, 40: 2}\n",
    "    for i in range(label_vec.shape[0]):\n",
    "        result[i] = key_dict[label_vec[i]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4901d618-d696-4c22-8585-8f56f32d610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keys_rev(pred_vec):\n",
    "    \"\"\" Take a vector of classifications and return keyboard outputs \"\"\"\n",
    "    result = torch.zeros_like(pred_vec)\n",
    "    key_dict = {0: -1, 1: 38, 2: 40}\n",
    "    for i in range(label_vec.shape[0]):\n",
    "        result[i] = key_dict[label_vec[i]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6673b37-2693-4d1d-b3e3-b00838146a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.Tensor(np.load('data/screenshots.npy'))\n",
    "labels = torch.Tensor(map_keys(np.load('data/command_keys.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd2e8d8-a81a-47af-a5ac-053c7aa9fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(labels)\n",
    "n_classes = len(np.unique(labels))\n",
    "n_pixels = images.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a184b58d-d62d-40da-ba7d-ee8aa5840380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15673 images in 3 categories, with 129600 pixels each.\n"
     ]
    }
   ],
   "source": [
    "print(f'{n_obs} images in {n_classes} categories, with {n_pixels} pixels each.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91e735f6-d928-45e0-9458-3ab92b89277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input vector: torch.Size([15673, 129600])\n",
      "Shape of targets: torch.Size([15673]) w/ unique values tensor([0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "# We want an input vector of 3760x(1450x288)\n",
    "print(f'Shape of input vector: {images.shape}')\n",
    "print(f'Shape of targets: {labels.shape} w/ unique values {torch.unique(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1d220-0ffb-42bc-84e9-38f0926e2431",
   "metadata": {},
   "source": [
    "#### Create DataSet class for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6605e614-ce99-4e32-8eb6-c76cfc35b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoImagesDataset(Dataset):\n",
    "    \"\"\" A Dataset class for processing screenshots\"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Give number of total observations in the dataset. \"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Return a training sample from the dataset. \"\"\"\n",
    "        label = self.labels[idx].type(torch.LongTensor)\n",
    "        image = self.images[idx].type(torch.FloatTensor)\n",
    "        #sample = {\"Image\": image, \"Label\": label}\n",
    "        return image, label\n",
    "    \n",
    "    def num_pixels(self):\n",
    "        \"\"\" Return the number of pixels in an image\"\"\"\n",
    "        return len(self.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed47d34-5605-415b-92c1-798ddd875c3f",
   "metadata": {},
   "source": [
    "#### Initialize dataset with new dino-game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15feb482-230d-407d-8a04-9c1d3b727ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_data = DinoImagesDataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6cd608-8b2e-4fe7-9c9a-f14397c492d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_obs: 15673, with 129600 pixels\n"
     ]
    }
   ],
   "source": [
    "print(f'N_obs: {dino_data.__len__()}, with {dino_data.num_pixels()} pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c82019-c8ca-4c9e-849c-4cf5252ec381",
   "metadata": {},
   "source": [
    "#### Setup train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcf2bbc-7c57-4e33-bb84-07b2533971f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12538 training images and 3135 testing images\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dino_data))\n",
    "test_size = len(dino_data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dino_data, [train_size, test_size])\n",
    "print(f'{train_size} training images and {test_size} testing images')\n",
    "dino_train_DL = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dino_test_DL = DataLoader(test_dataset, batch_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7ab70-4bea-4f9c-b902-dbed49d946f2",
   "metadata": {},
   "source": [
    "#### Create PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e7e26f-10db-411d-93c9-bc862326bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNNet(torch.nn.Module):\n",
    "    \"\"\" Basic NNet for playing the dino game \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size, 500)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(500, 250)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(250, 25)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.layer4 = torch.nn.Linear(25, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass on images to calculate log-probability of each key press given image pixels\"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        log_probs = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c4b30-6f4e-4ac8-9a44-56f33f65ab60",
   "metadata": {},
   "source": [
    "#### Instantiate and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a740bfb-bc31-4d16-b110-df578c241ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicNNet(\n",
      "  (layer1): Linear(in_features=129600, out_features=500, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=500, out_features=250, bias=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer3): Linear(in_features=250, out_features=25, bias=True)\n",
      "  (activation3): ReLU()\n",
      "  (layer4): Linear(in_features=25, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BasicNNet(n_pixels)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063b7593-922f-4798-8c6a-40e9e6ee44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "783883ef-8bc2-4771-9b06-dfae68c67aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 1.122\n",
      "[2,   200] loss: 0.720\n",
      "[3,   200] loss: 0.695\n",
      "[4,   200] loss: 0.700\n",
      "[5,   200] loss: 0.686\n",
      "[6,   200] loss: 0.697\n",
      "[7,   200] loss: 0.703\n",
      "[8,   200] loss: 0.708\n",
      "[9,   200] loss: 0.695\n",
      "[10,   200] loss: 0.708\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    running_loss = 0.0 # loss for current epoch\n",
    "    for i, data in enumerate(dino_train_DL):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero gradient (this is common practice with pytorch on each batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Now perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate gradients of loss function \n",
    "        loss.backward()\n",
    "        \n",
    "        # Backprop step to update params\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a763d9b-0c1d-4afc-bec1-644cd874dc6a",
   "metadata": {},
   "source": [
    "#### Evaluate models on out-of-sample (OOS) screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b37ded-3a31-4479-a014-0f41383e21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def test_accuracy(model):\n",
    "    \"\"\" Test accuracy of model \"\"\"\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dino_test_DL:\n",
    "            images, labels = data\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99985b99-9664-49a3-9785-4107aeb075e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.59649122807018"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87f25b-0b9e-4bed-a3c5-f64ae4ae1c37",
   "metadata": {},
   "source": [
    "#### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87fe1214-8767-445a-841c-8721612c9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'models/DNN.model')\n",
    "torch.save(model, 'models/nn.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-tools",
   "language": "python",
   "name": "dev-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
