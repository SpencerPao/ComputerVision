{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb3fbf3-eb58-4a63-bc58-adccbc6eb32b",
   "metadata": {},
   "source": [
    "# Dino Game model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10ea9d-7a20-4b89-a0ec-72766ec977e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summary\n",
    "* Initial go at dino game\n",
    "* Read-in Spencer's data and do some very minimal pre-processing to make things play nicely with popular ML pacakges like sklearn, keras, and PyTorch\n",
    "* Fit basic Softmax Regression model -- performs WAY better than I expected\n",
    "* Though this nice performance could be super misguiding -- could be a result of the data structure I mentioend above, or the fact that these samples aren't IID (one frame of the game relies on another) so we are inherently violating the assumptions of a LogReg model. For this, I suggest a \"thinning\" type of approach across even more dino-games if we want to learn an actual mapping\n",
    "* Will likely need a lot more data to train a deep-net, especially since LogReg performs so well.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* Spencer went all-out and sent massive image files in RGB channels (lol) so for now, I will just flatten these, drop that third channel (will mess up scaling of values in matrix though), and train on these vectors.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* A label prediction for the key to press (or what action the model should take given the pixel values in the image?)\n",
    "\n",
    "### Modeling task\n",
    "* Given an input image $X$, output a label for the action to be taken by the model (jump, duck, nothing)\n",
    "\n",
    "### Evaluation metric\n",
    "* Classification accuracy \n",
    "* Cross-entropy loss for training\n",
    "\n",
    "### Models\n",
    "* Multinomial logistic/Softmax regression\n",
    "* ConvNets (1D and 2D) -- Spencer suggest ResNet, but I have so little knowledge in this field, I kinda of want to do a \"survey\" first\n",
    "* SVM models?\n",
    "\n",
    "### To-do\n",
    "* Data pre-processing to make this task more \"learnable\" is needed. \n",
    "    * Images should be converted to grayscale -- models not learning well\n",
    "    * Perhaps move from predicting the integer value of the key-press and move to a one-hot encoding (I can write a utility function to go back and forth from the two to make the actual dinosaur move)\n",
    "   \n",
    "* More modeling\n",
    "     * Move to Google Collab for ConvNets -- LogReg was painfully slow on my local machine to fit\n",
    "     * Flat try fancy models next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65e5134-9a96-4b39-ae95-549873eb26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from typing import List\n",
    "import cv2 as cv\n",
    "# Using SMOTE for the over sampling portion.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3365cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta1 = os.path.join(Path(os.getcwd()).parent,'Window_capture\\\\Data\\\\command_keys_act.npy')\n",
    "sa1 = os.path.join(Path(os.getcwd()).parent,'Window_capture\\\\Data\\\\cleaned_data_act.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f278e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Command Keys Shape:  (1823,)\n",
      "Length Screenshot Shape:  (1823, 288, 1450)\n",
      "Screenshot Shape:  (288, 1450)\n"
     ]
    }
   ],
   "source": [
    "labels1 = np.load(ta1)\n",
    "images1 = np.load(sa1, allow_pickle = True)\n",
    "print(\"Length Command Keys Shape: \",labels1.shape)\n",
    "print(\"Length Screenshot Shape: \",images1.shape)\n",
    "print(\"Screenshot Shape: \",images1[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6673b37-2693-4d1d-b3e3-b00838146a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Command Keys Shape:  (12563,)\n",
      "Length Screenshot Shape:  (12563, 288, 1450)\n",
      "Screenshot Shape:  (288, 1450)\n"
     ]
    }
   ],
   "source": [
    "target_address = os.path.join(Path(os.getcwd()).parent,'Window_capture\\\\Data\\\\command_keys.npy')\n",
    "# screenshot_address = os.path.join(Path(os.getcwd()).parent,'Window_capture\\\\Data\\\\screenshots.npy')\n",
    "screenshot_address = os.path.join(Path(os.getcwd()).parent,'Window_capture\\\\Data\\\\cleaned_data.npy')\n",
    "\n",
    "labels = np.load(target_address)\n",
    "images = np.load(screenshot_address, allow_pickle = True)\n",
    "\n",
    "\n",
    "print(\"Length Command Keys Shape: \",labels.shape)\n",
    "print(\"Length Screenshot Shape: \",images.shape)\n",
    "print(\"Screenshot Shape: \",images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06758312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.imshow('Canny Image',images[0]) # We will use this.\n",
    "# cv.waitKey(0) # press any key to quit.\n",
    "# cv.destroyWindow('Canny Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888a9abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14386,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels0 = np.concatenate((labels, labels1))\n",
    "labels0.shape # Total # of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cc2f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, 38, 40]), array([12070,  1132,  1184], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels0, return_counts = True) # We see quite a bit of imbalance among the do nothing / jump / duck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d9804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10494,  7110,  2044, ...,  7381,  6506, 10714])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_list = [i for i, value in enumerate(labels0) if value == -1]\n",
    "idx = np.random.choice(len(res_list), 11000, replace=False) # Randomly choose X number of entries to be deleted specified as -1\n",
    "idx # indices to remove from the image dataset (that has the do nothing observations denoted as '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b616ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12563, 288, 1450)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape # Just taking a look at the number of osbervations from the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bfb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_flat = pd.DataFrame(images[:, :, :].flatten().reshape(images.shape[0], 417600))\n",
    "images_flat = images_flat.drop(images_flat.index[idx]) # flatten images then converted to dataframe for easier removal of idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673724d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 417600)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_flat.shape # Result. Total Rows - number of rows to be forgotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8deb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1823, 417600)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_flat1 = pd.DataFrame(images1[:, :, :].flatten().reshape(images1.shape[0], 417600)) # Jumps and ducks\n",
    "images_flat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9cb88c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3386, 417600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = np.vstack((images_flat, images_flat1))\n",
    "imgs.shape # Final Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a7e884f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3386,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels0 = np.delete(labels0, idx)\n",
    "labels0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "261404c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, 38, 40]), array([1492,  921,  973], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels0, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb201569",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labels0, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a18c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, 38, 40]), array([1140,  689,  710], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d2245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling the data\n",
    "smote = SMOTE(random_state = 101)\n",
    "X_train_samp, y_train_samp = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a53743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, 38, 40]), array([1140, 1140, 1140], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_samp, return_counts = True) # Oversampled Balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa446ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\spenc\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit LogReg model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_samp,y_train_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5014ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg accuracy on held-out frames = 0.9764\n"
     ]
    }
   ],
   "source": [
    "y_hat = log_reg.predict(X_test)\n",
    "print(f'LogReg accuracy on held-out frames = {round(accuracy_score(y_test, y_hat),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c5206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     nothing       0.97      0.99      0.98       352\n",
      "          up       0.96      0.96      0.96       232\n",
      "        down       0.99      0.98      0.98       263\n",
      "\n",
      "    accuracy                           0.98       847\n",
      "   macro avg       0.98      0.97      0.98       847\n",
      "weighted avg       0.98      0.98      0.98       847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat, labels=[-1, 38, 40])\n",
    "target_names = ['nothing', 'up', 'down']\n",
    "print(classification_report(y_test, y_hat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc8a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(log_reg, open('Existing_Models/log-reg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420931c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ed2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7cb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3d039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891464a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee014e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4364a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297b698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792d0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90d9e9-9731-47c7-8c2e-5346a9804b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_flat = images[:, :, :, 0].flatten().reshape(3760, 417600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e735f6-d928-45e0-9458-3ab92b89277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want an input vector of 3760x(1450x288)\n",
    "print(f'Shape of input vector: {images_flat.shape}')\n",
    "print(f'Shape of targets: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c840fae-41ef-4a00-b3a3-acb00d34a452",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### View the actual frames from the screen capture, and the corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18238b-97eb-4701-bfe2-bf612f5f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the idx'th frame\n",
    "idx = 10\n",
    "plt.rcParams['figure.figsize'] = [25, 10]\n",
    "print(f'Label= {labels[idx]}')\n",
    "plt.imshow(images[idx], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1d220-0ffb-42bc-84e9-38f0926e2431",
   "metadata": {},
   "source": [
    "#### Split into train-test splits for now (will do train-dev-test next week when I get into hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9792c-b253-4efd-8798-e952da9aa9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images_flat, labels, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c27bf2-c06b-45bb-98a8-0b9daa15a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(X_train)} training examples')\n",
    "print(f'{len(X_test)} testing examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449478c-62cb-410a-8b4a-46bb1e813e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LogReg model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63892325-8ec2-412d-9b85-02efe886b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedff85-5839-4447-8ff3-ed2f24fcd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LogReg accuracy on held-out frames = {round(accuracy_score(y_test, y_hat),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22acf962-5814-4ffe-9b73-de9720932f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_hat, labels=[32, 38, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d58a0-df30-47eb-a32d-f87a8d98eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['start', 'up', 'down']\n",
    "print(classification_report(y_test, y_hat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe1214-8767-445a-841c-8721612c9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(log_reg, open('log-reg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e9e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
